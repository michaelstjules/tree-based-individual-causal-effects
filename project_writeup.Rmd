---
title: "CM 764 - Project: Evaluating BART and Synthetic Tree-Based Methods for the Estimation of Individual Causal Effects"
author: "Michael St. Jules"
date: "April 2017"
output: pdf_document
bibliography: project_writeup.bib
header-includes:
- \usepackage{graphicx}
- \usepackage{color}
- \usepackage{hyperref}
- \usepackage{epic}
- \usepackage{amssymb, amsfonts, amsmath, textcomp, enumerate, amsthm, tikz}
- \PassOptionsToPackage{pdfmark}{hyperref}\RequirePackage{hyperref}
- \usepackage{float}
- \newcommand{\tr}[1]{{#1}^{\mkern-1.5mu\mathsf{T}}}
- \renewcommand{\bf}[1]{\mathbf{#1}}
- \newcommand{\comment}[1]{}
---

```{r, echo=FALSE}
load("~/Desktop/CM 764/Project/data_variables.RData")
```

```{r, echo=FALSE, eval=FALSE}
"- Other randomForestSRC methods??? [@individual]: Bivariate imputation approach, HonestRF, just CF
- linear regression, propensity score weighting or matching methods? See [@Hill].
- TMLE? CausalTrees/CausalForests? [@CMGP]? [@Sontag]? Need to generalize from binary treatment. Couldn't find calCause
- prediction interval coverage?? Too late. Some methods not ready for it?
- different treatment assignment probabilities?
- causaldrf: does hi_est do individual dose-response?
- causaldrf: hi_sim_data for simulated data
- Compare to methods with access to random unbiased sample of dataset?
- Calculate importances (`relative.influence`) of covariates?
- How to evaluate/cross-validate without access to missing counterfactuals? Not necessary for BART?
-Need interaction between treatment and covariates, so either trees with depth at least 2, or add interaction variables to the model. 
-Take logarithms of some positive data? Only responses, doesn't matter for baseline covariates? Trees are robust to monotone transformations of any individual covariate (just apply the transformation to the split points to get an identical tree). Most of the baseline covariates are known to be normally distributed in the general population. BART isn't robust to monotone except with quantile splitting????"
```

\section{Abstract}

Bayesian Additive Regression Tree (BART) and Synthetic Random Forest methods are evaluated for the estimation of individual causal effects with nonbinary treatments, and on data with as little simulation as possible. For this purpose, data from a small randomized, cross-over, controlled trial is used. In general, of those methods evaluated, BART performed the best on the dataset.


\section{Introduction and Background}

\subsection{Causal Inference}

\emph{Causal inference} is the field of statistics concerned with causality, with two broad goals: 
\begin{enumerate}[(i)]
\item model selection, or estimating the parameters of distributions over random variables and which ones \emph{cause} which variables, formalized as a directed edge from causes to their effects in a directed acyclic graph, and 
\item the estimation of causal effects, i.e. the expected value of one random variable given that some are \emph{set} to specific values, as a function of these values (and possibly others conditioned on).
\end{enumerate}

Both are important in science, but the latter is crucial in determining which course of action to take in applications, e.g. which medical treatment to prescribe (if any at all) or what policies for a government to adopt. The ``gold standard" in establishing causal effects in science is the randomized trial, an experiment, but for financial, time-constraint or ethical reasons, randomized trials may not be feasible. As such, observational studies may be preferred, and causal inference in this setting is much more challenging. This is the setting this paper is generally concerned with. 

This project is formulated within the \emph{Neyman-Rubin causal model} for counterfactual inference, generalized to the nonbinary treatment case as in [@continuous]. It consists of the following random variables:
\begin{itemize}
\item \emph{subjects} (or \emph{individuals}, \emph{units}, \emph{contexts} or \emph{patients}) ${\bf X} \in \mathcal{X}$ (not to be confused with the data matrix, notation which I also use later)
\item \emph{treatments} (or \emph{interventions}) ${\bf T} \in \mathcal{T}$
\item \emph{potential outcomes} (or \emph{counterfactuals}) $Y_{\bf t}$ for all ${\bf t} \in \mathcal{T}$. 
\end{itemize}
Only $Y_{\bf T}$ is observed, i.e. we do not have access to the counterfactual outcomes $Y_{\bf t}$ for which ${\bf t} \neq {\bf T}$. This is \emph{the fundamental problem of causal inference}. Our goal, then, is to estimate $Y_{\bf t}$ for these other values of ${\bf t}$. This is significantly more challenging with observational data, with individuals choosing their own treatments, than in the experimental setting, where subjects are randomly and independently assigned to treatment groups.

Although much work has been done in estimating \emph{average treatment effects} (ATEs, or average causal effects, ACEs) in the binary treatment setting and some work on \emph{average dose-response functions} (e.g. with propensity scores [@continuous]), i.e. the quantities 
\[ \mathbb{E}[Y_1]-\mathbb{E}[Y_0] \text{ and } \mathbb{E}[Y_{\bf t}],\]
respectively, functions of $\bf t$ only, we are interested in the outcomes for \emph{individuals} (e.g. individual treatment effects or ITEs, or individual causal effects, ICEs, and indidividual dose-response functions), i.e. \[\mathbb{E}[Y_{\bf t}|{\bf X} ={\bf x}],\]
a function of both $\bf x$ and $\bf t$. Being able to estimate these accurately could, for example, lead to further personalized and automated medicine. In the binary treatment setting, the two treatments groups are referred to as \emph{treatment}, with label 1, and \emph{control}, with label 0. 

In the \emph{Neyman-Rubin causal model}, some assumptions are needed for these random variables. The first is \emph{weak unconfoundedness} [@continuous]: ${\bf x}, {\bf T}, Y_{\bf t}$ are said to be \emph{weakly unconfounded} if 
\[Y_{\bf t} \ \bot \ {\bf T} \ | {\bf X} \ , \ \text{ for all } {\bf t} \in \mathcal{T} . \]
That is, after controlling for ${\bf x}$ (potential confounders), the potential outcomes and the particular treatment received are independent. It is also referred to the \emph{no hidden confounders} assumption. 

Unfortunately, this assumption is not technically verifiable in practice in the observational setting, since, of course, $Y_{\bf t}$ is not observed for all $\bf t \in \mathcal{T}$. My simulation in this project, however, will satisfy it trivially, since the outcomes we sample $\bf t$ conditionally on 

A second assumption is that each pair ${({\bf x}, {\bf t}) \in \mathcal{X} \times \mathcal{T}}$ must be possible, i.e. has a positive probability or a positive density. 


\subsection{Overview}

The methods I evaluated are those which seemed most promising in [@individual], judging by Figure 1 in that paper: Bayesian Additive Regression Trees (BART, with the function `bart` from the R package `BayesTree` [@RBART]) and synCF (with the function `rfsrcSyn` from the R package `randomForestSRC` [@RSynth]). I also evaluated some slight modifications to these methods. The use of BART for causal inference is discussed in detail in [@Hill], which also mentions the possibility of its use for individual treatment effects. BART was one of the three overall winners of a recent causal inference competition, also performing the best on the individual prediction task. [@competition] Other methods in [@competition], [@Sontag], [@CMGP], [@causalTree], [@causalForest] have also been used in the estimation of individual treatment effect, but only for binary treatments, and much of the code has been made specific to binary treatments. It is beyond the scope of this project to generalize them. 

I present some background on BART and synthetic forests before proceeding to the experiments. Of those methods evaluated, BART seemed to have performed the best. 


\subsection{BART}

\emph{Bayesian and Additive Regression Trees} (\emph{BART}) [@BART] is a sum of trees regression model, which uses boosting with the Bayesian regularized trees from which it is composed. Its application to the estimation of causal effects is discussed first in [@Hill]. The model is written as 
\[ Y = \sum_{j=1}^m f_j({\bf x }) + \epsilon = \sum_{j=1}^m g({\bf x }; T_j, M_j) + \epsilon \ , \ ~~~~~~~~~~~ \ \epsilon \sim \mathcal{N}(0,\sigma^2) \ , \]
where each $f_j = g(-; T_j, M_j)$ is the predictor of a binary decision tree $T_j$ and $M_j = \{ \mu_{kj} | k \}$ is the set of values assigned to each terminal node, with $\mu_{kj}$ assigned to terminal node $k$. By default `ntree` = $m=200$. A tree $T_j$ may have depth $\geq 2$ to capture interactions between covariates. To regularize the model, data-informed priors are put on the parameters of the $T_j, M_j$ and $\sigma$, and the goal is to be able to sample a model from the posterior
\[ p((T_1, M_1), \dots, (T_m, M_m), \sigma | {\bf X, y}) \ ,  \]
to apply to new data $\bf x$. The prior is factored as
\[ p((T_1, M_1), \dots, (T_m, M_m), \sigma | {\bf X, y}) = \prod_{j=1}^m \bigg[ \Big( \prod_k p(\mu_{kj} |T_j) \Big) p(T_j) \bigg] p(\sigma) \ .  \]
Here, $p(T_j)$ is determined by 
\begin{enumerate}[(i)]
\item the probability that a node at depth $d \geq 0$ is nonterminal, decreasing as $d$ increases
\item the distribution on the splitting variable assignments at each interior node, chosen by default (`usequants = FALSE`) to be uniform over the range of values taken by a variable, or uniform over the quantiles of that variable, i.e. uniform over the averages of pairs of successive values in the data (`usequants = TRUE`); and 
\item the distribution on the splitting rule assignment in each interior node, conditional on the splitting variable, chosen by default to be uniform over the discrete set of splitting values.
\end{enumerate}

The other priors are chosen as $p(\mu_{kj} |T_j) = \mathcal{N}(\mu_{kj} \ ; \ \mu_\mu, \sigma_\mu)$ and $p(\sigma)$ so that $\sigma^2$ is inverse chi-squared, to introduce conjugacy structure for ease of computation, with their hyperparameters estimated from the data. 

```{r, echo=FALSE, eval=FALSE}
#, given by
#\[ \alpha (1+d)^{-\beta} \ , \  ~~~~~ 0 < \alpha < 1, \beta \geq 0 \ ; \]
#$p(\mu_{kj}|T_j)$ and $p(\sigma)$ are conjugate normal and inverse chi-squared for $\sigma^2$. 
```

The model is then fit by a \emph{Bayesian backfitting Markov chain Monte Carlo algorithm}, a Gibbs sampler, by sampling $(T_j, M_j)$ conditional on all other variables (including $\bf y$), for each $j$, and $\sigma$ conditional on all other variables (including $\bf y$), and repeating this until convergence. In particular, sampling $(T_j, M_j)$ can be done conditional only on the partial residual ${\bf R}_j = {\bf y } - \Big(\sum_{j' \neq j} g({\bf x}_i); T_{j'}, M_{j'} \Big)_{i=1}^N$ and $\sigma$. 

```{r, echo=FALSE, eval=FALSE}
#\[(T_j, M_j) \ | \ {\bf R}_j = {\bf y } - \Big(\sum_{j' \neq j} g({\bf x}_i); T_{j'}, M_{j'} \Big)_{i=1}^N \ , \sigma \]
#for $j=1, \dots, m$, and then
```

To predict the response for a new data point $\bf x$, we simply look at the prediction of one of the models near convergence (or multiple of them, and take their average). 

The method I call `BART` below is simply BART with default values, in particular `usequants = FALSE`. `BART2` refers to BART with default values but quantile splitting, i.e. `usequants = TRUE`. I would have expected `BART2` to perform better, since `BART`'s uniform range splitting seems naive, and quantile splitting on the binary treatment variables should make no difference. However, this was not observed in the experiments, as `BART` generally performed better than `BART2`. 


\subsection{Synthetic forests}

A synthetic forest [@synthetic] is a random forest precomposed with several other random forests. In particular, given a dataset $({\bf X}, {\bf y}) = (\{{\bf x}_i\}_{i=1}^N, \{y_i\}_{i=1}^N)$ and a new data point to make a prediction on, ${\bf x}$, a synthetic forest is a random forest which takes as input training data $({\bf X}, {\bf y})$, and new data ${\bf x}$, as well as the predictions, called \emph{synthetic features}, made by several other random forests with different hyperparameters on this new data ${\bf x}$. In particular, the use of synthetic forests can replace hyperparameter tuning in random forests, but even more than this, it can choose predictions from the different random forests \emph{locally} and \emph{adaptively}. Synthetic forests are implemented by the function `rfsrcSyn` in the R package `randomForestSRC` [@RSynth]. 

A random forest is made up of `ntree` trees, with nodes of size at least `nodesize`$\geq 1$. At each node that can be split and still satisfy the `nodesize` constraint, a random subset of $1 \leq$`mtry` $\leq p$ features are chosen, where $p$ is the total number of features, and the best split among splits using any of those `mtry` features is performed. 

For synthetic forests, random forests are first constructed with `ntree` trees each, where by default `ntree`=1000, one forest for each value in a sequence of different node sizes, the argument `nodesizeSeq`, by default `c(1:10,20,30,50,100)`. The ceiling of $p/3$ is used for the `mtry` values of the random forests, but a sequence `mtrySeq` of different values can instead be supplied (each value paired with each `nodesizeSeq` value). Node size is prioritized here since it acts as "a type of bandwidth smoothing parameter", with larger node sizes smoothing the predictions.

Then, another random forest is constructed, taking as input the dataset and the predictions of the previous random forests on the new data as input. It uses the same `ntree` value, but its own `nodesize` and `mtry` values. By default, `nodesize=5` and `mtry`=`ceiling`$(p+$`length(nodesizeSeq)`$)/3$, i.e. a third of the new number of features. 

The data used in this project only has about 34 data points (depending on missing responses), so the larger `nodesizeSeq` default values will yield trivial trees. The trees themselves are constructed on bootstrap samples of the data. 

The method I call `synth` here is just `rfsrcSyn` with default values. An important issue with `synth` here, however, is that it does not reweight treatment groups, so that any group which is underrepresented may be effectively ignored by never or rarely splitting on the treatment variable(s). As such, one might expect `synth` to have a relatively large average squared bias, and this does seem to be the case in my experiments (compard to `BART` and `synCF`, which I describe next). 

Also using `rfsrcSyn` is the method `synCF`, which is a simple adaptation of the same method by the same name in [@individual] to 4 different treatments. I construct separate predictors for each treatment group, depending only on the data from the corresponding treatment group, and for prediction only on new data from the same treatment group. However, when no subject is in a particular treatment group for the given sample, I just use all of the data for that particular group. 

This method may be less suitable for a large number of possible treatments, and especially continuous treatment variables, although in the latter case, treatments may potentially be grouped into a small number of intervals or blocks. In particular, we could use random or greedily chosen blocks, or perhaps equivalently, just use trees which start by splitting only on the treatment variables several times, before then splits on other variables (possibly again including the treatment variable). This is left to future work. 

\bigskip

\subsection{BART with synthetic features}

I also implemented BART with synthetic features as in `synth`, i.e. the algorithm is exactly the same, with multiple random forests producing synthetic features, but instead of passing these synthetic features to another random forest, they are passed to BART. One might expect these synthetic features to overwhelm BART, with BART splitting on them at the (relative) expense of splitting on the treatment variable, and furthermore, the synthetic features themselves did not come from treatment-balanced algorithms, i.e. the treatment groups were not reweighted. Together, one should expect this algorithm to be more biased than `BART` alone, but less biased than `synth`, which is observed in the experiments. With default values (and `usequants = FALSE`), I call this method `BARTsynth`, and with default except for `usequants = TRUE`, I call this method `BARTsynth2`. 




\section{Experiments}

I start with a randomized, cross-over, controlled trial [@study], i.e. a study in which each subject received multiple treatments separately, with responses recorded for each treatment. Under the assumption that earlier treatments do not affect responses from later treatments, a subject's multiple responses may be treated like different potential outcomes, pretending that they only actually received one of the treatments. The only data which is simulated is the treatment assignment: each subject $i$ received several treatments, but, according to some non-uniform probability distribution over treatments conditional on the characteristics of subjects, ${\bf x}_i$, we keep only the data corresponding to \emph{one} of their treatments, say ${\bf t}_i^*$, so that ${\bf t}_i^* \sim p({\bf T} | \bf{ x}_i) = P({\bf T} | {\bf X } = {\bf x}_i)$ for the treatment assignment distribution $p({\bf t}|{\bf x})$, which is the \emph{generalized propensity score} [@continuous] for our simulated data. That is, we keep only $({\bf x}_i, {\bf t}_i^*, y_i({\bf t}_i^*))$, where $y_i({\bf t})$ is the response of subject $i$ to treatment ${\bf t}$. Hence, the training dataset is semi-simulated while the test set comes directly (or with minor modification) from the study. The goal is then, with this subset of the data, to predict each subject's responses to all of the treatments they received in the trial. As far as I am aware, this is the first use of a crossover study to evaluate methods for the estimation of individual treatment effects with access only to one potential outcome per subject (as is usually the case in observational studies). As such, this evaluation relies neither on full simulation as in [@individual] nor on the use of nearby surrogates for potential outcomes as in [@Sontag]. In future works, the treatment assignment distribution may come from a generalized propensity score estimated from a different study involving the same treatment and many of the same covariates. One other major difference between this project and [@individual] is the size of the training set: mine is about 34, while [@individual] use 500 and 5000. 

Although initially interested in studying dose-effects with a continuous treatment variable, I've been unable to find such a crossover study, and have settled for a crossover study with each subject receiving 3 or 4 of at most 4 possible treatments was used, where the 4 possible treatments are pairs of binary variables [@study]. In this study, the treatments are exposure and position, (`EXP, POS`), denoting exposure (`EXP=1`) and non-exposure to pepper spray (`EXP=0`), and restrained prone (`POS=1`) and sitting (`POS=0`) positions after exposure (or non-exposure), and the responses of interest for this project are heart rate, respiratory rate, blood pressure (arterial, systolic and diastolic), tidal volume at 1 minute or 3 minutes after treatment, although various other measurements are taken in the study. The covariates of interest (besides treatment) are baseline measurements for the responses of interest, as well as demographic information, like age, sex, ethnicity, history of tobacco use, medical history, history of medication use, height, weight and body mass index. The assignemnt distribution is defined as $p({\bf t}|{\bf x}) = p$(`EXP`, `POS`$|{\bf x}) = p$(`EXP`$|x)p($`POS`|`EXP`,${\bf x})$, where $\log p$(`EXP=1`$|{\bf x})$ and $\log p($`POS=1`|`EXP`,${\bf x})$ are linear in the variables being conditioned on and some interaction terms. The average (over all subjects) probability of `EXP=1` is around 0.11, while that of `POS=1` is around 0.26. 

The measures used to evaluate each method are the \emph{average squared bias} (which I often just call bias), the \emph{average variance} and their sum, the \emph{average prediction squared error}, or \emph{APSE}. In this setting, the mean of each response is just the unique response (no ${\bf x}_i$ is repeated), so the variances of the responses are taken to be 0. In the causal inference literature [@Hill], the estimation of heterogeneous effects (PEHE) for binary treatments may be used, and this is just the MSE, mean squared error, or RMSE, root mean squared error, for the difference between treated and control responses for each subject, i.e. the average squared difference or the root of this, with the average taken over the values to be predicted. 

In this project, I use the default values for all algorithms, unless otherwise specified (as described with `usequants`). Prediction is also done within sample rather than out-of-sample, i.e. all subjects are used for learning with only one treatment each, and predictions are made on the same subjects, for all treatments they received in the cross-over study, rather than separating the subjects into training and test subjects, and making predictions on test subjects, for all available treatments. Out-of-sample prediction errors would be interesting to compare methods with in future work. For this small dataset with $N=34$ subjects, $k$-fold cross-validation for $k \geq 5$ (or just leave-one-out cross-validation with $k=N$) would be preferable, to not spread the data to thinly. 

\subsection{Preparing the Data and Treatment Assignment}

```{r}
library(haven)
data <- 
  read_por("~/Desktop/CM 764/Project/Pepper_spray/ICPSR_02961/DS0001/02961-0001-Data.por")
# fix directory so it doesn't depend on my computer

# For each subject, replace their baseline vital measurements 
# (they repeat the measurements before each trial)
# with their minimum baseline over all trials. 
# This should hopefully take care of some treatment order effects

# The covariates are:
#BTV       "BASELINE TIDAL VOLUME"                                         
#BRR       "BASELINE RESPIRATORY RATE"                                     
#BHR       "BASELINE HEART RATE"  
#BSBP      "BASELINE SYSTOLIC BLOOD PRESSURE"                              
#BDBP      "BASELINE DIASTOLIC BLOOD PRESSURE"                             
#BMAP      "BASELINE MEAN ARTERIAL PRESSURE"

# The covariates (dependent variables) will consist of the above (modified), and 
#AGE       "AGE OF SUBJECT"
#SEX       "GENDER OF SUBJECT"
#ETH       "ETHNICITY OF SUBJECT"
#HT        "SUBJECT'S HEIGHT (IN METERS)"
#WT        "SUBJECT'S WEIGHT (IN KILOGRAMS)"
#BMI       "BODY MASS INDEX (KG/M2)"
#PMH       "PAST MEDICAL HISTORY"
#TOB       "TOBACCO USE HISTORY"
#MED       "HISTORY OF MEDICATION USE"

for (subj in data$SUBJ){
  data$BTV[data$SUBJ==subj] <- min(data$BTV[data$SUBJ==subj])
  data$BRR[data$SUBJ==subj] <- min(data$BRR[data$SUBJ==subj])
  data$BHR[data$SUBJ==subj] <- min(data$BHR[data$SUBJ==subj])
  data$BSBP[data$SUBJ==subj] <- min(data$BSBP[data$SUBJ==subj])
  data$BDBP[data$SUBJ==subj] <- min(data$BDBP[data$SUBJ==subj])
  data$BMAP[data$SUBJ==subj] <- min(data$BMAP[data$SUBJ==subj])
}

# View(data)

x.test <- data[c("SUBJ", "AGE", "SEX", "ETH", "HT", "WT", "BMI", "PMH", "TOB", "MED",
                 "BTV", "BRR", "BHR", "BSBP", "BDBP", "BMAP")] #baseline covariates
# but also SUBJ, for convenience, but SUBJ will be removed later
x <- unique(x.test) #get rid of duplicated rows

#test data x values to produce predicted y's
x.test <- cbind(x.test[,names(x)!="SUBJ"], EXP=data$EXP, POS=data$POS)
```



```{r}
# View the histograms for the baseline covariates corresponding to response covariates.
# We want to predict the response on the same power-scale as the corresponding
# baseline covariate, since rather than applying power transformations 
# guided by the skew of the response, which may be the result of the biased 
# treatment assignment and lead to poor generalization,
# I check for skew in the corresponding baseline covariates. 
# Furthermore, many of these variables have been observed to be roughly 
# normally distributed in the general population.
par(mfrow=c(2,3))
hist.default(x$BRR) #pretty well normal
hist.default(x$BHR) #symmetric but possibly two-modal
hist.default(x$BSBP) #pretty well normal
hist.default(x$BDBP) #pretty well normal
hist.default(x$BMAP) #pretty well normal
hist.default(x$BTV) #slightly right-skewed (right-tailed)
# a power transform of ~0.5 would fix this
hist.default(x$BTV^0.5)
```



Now, define the simulated treatment assignment mechanism and how to sample from the data:
```{r}
# First, some global variables to avoid recomputing
max.AGE <- max(x$AGE)
min.AGE <- min(x$AGE)
max.WT <- max(x$WT)
min.WT <- min(x$WT)
#the log probability of EXP=1 will be linear (affine) in the following
exponent <- 3*(max.AGE-x$AGE)/(max.AGE-min.AGE) + (x$WT-min.WT)/(max.WT-min.WT) + 
  5*(x$SEX==1) + 3*(x$ETH==2) + 2*(x$ETH==3) + 5*(x$TOB == 2) +
  3*((max.AGE-x$AGE)/(max.AGE-min.AGE)+1)*(3*(x$ETH==2)+2*(x$ETH==3))*
  (3*(x$SEX==1)+1)*(x$TOB == 2)
max.exponent <- max(exponent)
min.exponent <- min(exponent)
#i.e. log p(EXP=1) = a*exponent+b
#want max prob of EXP==1 to be 1/5, min to be 1/18, so fit a line:
#slope
a.EXP = (log(1/5)-log(1/18))/(max.exponent-min.exponent)
#intercept
b.EXP = log(1/5) - a.EXP*max.exponent

#log p(POS=1|EXP) = a*(exponent+2*EXP)+b
#want max prob to be 3/4, min to be 1/10
#slope
a.POS_EXP = (log(3/4)-log(1/10))/(max.exponent+2-min.exponent) #2 for 2*EXP
#intercept
b.POS_EXP = log(3/4) - a.POS_EXP*(max.exponent+2) #2 for 2*EXP

pEXP1 <- function(){
  exp(a.EXP*exponent+b.EXP)
}

pPOS1_EXP <- function(EXP){
  exp(a.POS_EXP*(exponent+EXP)+b.POS_EXP) #this was supposed to be
  #exponent+2*EXP, but it's too late to fix now
  #the distribution below is still a valid distribution
}

#sample treatments for each subject
treatment_dist <- function(x){
  x.EXP <- as.integer(runif(nrow(x)) <= pEXP1())
  x.POS <- as.integer(runif(nrow(x)) <= pPOS1_EXP(x.EXP))
  data.frame(EXP=x.EXP,POS=x.POS)
  # At least three possibilities for dealing with missing treatments in data:
  # (1) not care that some responses will be missing for some treatments (OK for trees?)
  # (2) keep reassigning until a valid treatment is obtained
  # (3) "round" to the nearest treatment:
  # If (0,0) or (1,1) is obtained but missing, flip a coin between (1,0) and (0,1),
  #  favouring (0,1) (e.g. 2/3) ?
  # If (0,1) or (1,0) is obtained but missing, flip to the other
  
  # For now, I'm using (1)
}


getSample <- function(y_name="RR_1", x.=x, data.=data, 
                      t=NA, treatment_dist.=treatment_dist){
  if(is.na(t)){
     t <- treatment_dist.(x.)
  }
  y <- numeric(nrow(x.))
  y[] <- NA #fill with NAs
  j <- 1 #index in x
  #note that rows appear in the same order (increasing by SUBJ) in both x and data
  for(i in 1:nrow(data.)){
    if(data.[i,"SUBJ"]==x.[j,"SUBJ"] & all(data.[i, c("EXP","POS")] == t[j,c("EXP","POS")]))
      y[j] <- as.double(data.[i,y_name])
    if(i < nrow(data.) & data.[i+1, "SUBJ"] != x.[j,"SUBJ"]){ 
      j <- j+1
    }
  }
    
  cbind(x.[,names(x.) != "SUBJ"],t,y) #remove SUBJ
  #should x and t be combined into one variable?
}

```

At the very least, it's clear that the treatment assignment is not uniform, with some treatment groups much larger than others on average:

```{r}
# marginal probability of EXP=1 (i.e. being pepper sprayed)
mean(sapply(1:5000, FUN=function(j){mean(treatment_dist(x)$EXP)}))
# marginal probability of POS=1 (i.e. being restrained)
mean(sapply(1:5000, FUN=function(j){mean(treatment_dist(x)$POS)}))
```



\subsection{Results}

We consider two partial orders on estimators here:
\begin{itemize}
\item we say estimator $\tilde{A}$ is \emph{weakly preferred} to estimator $\tilde{B}$ if the average prediction squared error (APSE) of $\tilde{A}$ is strictly less than that of $\tilde{B}$. 
\item We say estimator $\tilde{A}$ is \emph{clearly preferred} to estimator $\tilde{B}$ if both the average squared bias of $\tilde{A}$ is at most that of $\tilde{B}$, and the average variance of $\tilde{A}$ is at most that of $\tilde{B}$; and one of these inequalities is strict (or $\tilde{A}$ is weakly preferred to $\tilde{B}$)
\end{itemize}
In the following a \emph{clear winner} is an estimator with both the least average squared bias and the least average variance, or one for which no other estimator is clearly preferred to it. A clear winner may not always exist. A \emph{weak winner} is an estimator with the least APSE, or one for which no other estimator is weakly preferred to it.

The following graph and estimates were produced with 100 samples:

\includegraphics[width=0.5\textwidth]{HR_1_bias_variance.pdf}
```{r, echo=FALSE}
print(bias.variance.APSE)
```
BART is the clear winner here. BART2, BARTsynth and BARTsynth2 performed significantly worse than BART, and synth and synCF performed significantly worse still. 

The following graphs and estimates were produced with 50 samples each:

\includegraphics[width=0.5\textwidth]{RR_1_bias_variance.pdf}
```{r, echo=FALSE}
print(bias.variance.APSE.RR_1)
```
BART has the least average squared bias here by a fairly large margin, but a slightly higher variance than BARTsynth, so there is no clear winner. BART is a weak winner, and by a decent margin. synCF had a significantly higher variance than the others. 

\includegraphics[width=0.5\textwidth]{DBP_3_bias_variance.pdf}
```{r, echo=FALSE}
print(bias.variance.APSE.DBP_3)
```
The are no clear winners here, but as BART, BART2 and BARTsynth2 are all close in average squared bias and average variance. Between them, APSE(BART2) < APSE(BART) < APSE(BARTsynth2), so BART2 is the weak winner. 


\includegraphics[width=0.5\textwidth]{MAP_3_bias_variance.pdf}
```{r, echo=FALSE}
print(bias.variance.APSE.MAP_3)
```
There is no clear winner here, but BART, BART2 and BARTsynth have no estimators clearly preferred to them. BARTsynth has the least variance, but there's a large trade-off, giving it a much larger bias. BART is the weak winner, followed closely by BART2. 


\includegraphics[width=0.5\textwidth]{SBP_3_bias_variance.pdf}
```{r, echo=FALSE}
print(bias.variance.APSE.SBP_3)
```
There is no clear winner here, but BART is almost one and has the least APSE by a decent margin. BART is the weak winner. 



\includegraphics[width=0.5\textwidth]{TV_1_bias_variance.pdf}
```{r, echo=FALSE}
print(bias.variance.APSE.TV_1)
```

BART here is the clear winner. BART2 is clearly preferred to BARTsynth2, although the difference in variance may be smaller than the difference in sampling, and neither is clearly comparable to BARTsynth, but both are weakly preferred to it (lower APSE). synth and synCF have the worst APSE, and synCF is particularly bad because of its high variance. 

In the last experiment, BTV is replaced with BTV^0.5; and TV_1, with TV_1^0.5: 

\includegraphics[width=0.5\textwidth]{TV_1.5_bias_variance.pdf}
```{r, echo=FALSE}
print(bias.variance.APSE.TV_1.5)
```

synCF here is actually the least biased, but it has the worst variance, so it is not clearly comparable to any of the others. However, it does have the worst APSE, so all others are weakly preferred to it. BART, BART2, BARTsynth2 and synCF are not clearly comparable to one another, but in terms of APSE, from lowest/best to highest/worst, we have: BART, BART2, BARTsynth2, BARTsynth, synth, synCF. 

\bigskip

Note that in all of the above experiments the average squared bias was always much greater (over 5 times greater in most cases) than the average variance. I suspect the main reason for this is because I defined the expected value of the response for individuals with covariates ${\bf x}_i$ to be $y_i$, i.e. the unique response corresponding to those particular covariates, and in doing so, the average squared bias absorbed the average variance of the response $y$ which would have been observed under repitition of the experiment with the same subjects, setting it to 0 here. That is, the bias term is computed from differences between the average predicted value and a \emph{noisy} response. Rather than using the responses directly, their values could have been smoothed locally. 

Another possible reason is how few subjects are actually in the study: about 34, depending on whether some have missing responses or not.

\bigskip

All together, BART was a weak winner in all but one case (`BP_3`), in which it followed closely in second. BART was also a clear winner in two casse (`HR_1, TV_1`), and \emph{nearly} a clear winner in two other cases (`RR_1, SBP_3`). synCF almost always performed poorly due to its high variance, and synth only performed well on `SBP_3`. synth was usually more biased than BART, BART2 and synCF, as might be expected. BART2 and BARTsynth2 performed well in comparison, and BARTsynth only slightly worse than BART2 and BARTsynth2. As such, for the particular dataset in this project, I would order the estimators from best to worst the following way:

\begin{enumerate}
\item BART
\item BART2 and BARTsynth2
\item BARTsynth
\item synth
\item synCF
\end{enumerate}

In terms of computation time, the BART-based methods were a couple of times slower. 


\section{Conclusion}

In this project, I compared the two main contenders, BART and synCF (and some variants), in [@individual] in for the estimation of individual causal effects on a small minimally-simulated dataset. My evaluation lead to the opposite ranking between them: in [@individual], synCF performed slightly better than BART, but here, BART performed better than synCF, and sometimes by quite large margins. 

Directions for future work include:
\begin{itemize}
\item out-of-sample evaluations, 
\item continuous treatment variables, and
\item the adaptation of other methods currently implemented only for binary treatments to nonbinary treatments.
\end{itemize}









```{r, echo=FALSE, eval=FALSE}
# Export -> Save as PDF -> 5x7 in
bias.variances <- t(cbind(bias.variance.BART, bias.variance.BART2, bias.variance.synth, bias.variance.synCF, bias.variance.BARTsynth, bias.variance.BARTsynth2))
names <- c("BART", "BART2", "synth", "synCF", "BARTsynth", "BARTsynth2")
rownames(bias.variances) <- names
colnames(bias.variances) <- c("bias2", "variance")
plot(bias.variances, xlab="average squared bias", main="variance vs bias of estimators predicting HR_1", xaxp=c(floor(min(bias.variances[,1]))-2, ceiling(max(bias.variances[,1]))+3, 10))
text(bias.variances, labels = names, pos = c(4,4,1,4,4,1))
bias.variance.APSE = cbind(bias.variances, APSE=bias.variances[,1]+bias.variances[,2])
```

```{r, echo=FALSE, eval=FALSE}
bias.variances.DBP_3 <- t(cbind(bias.variance.BART.DBP_3, bias.variance.BART2.DBP_3, bias.variance.synth.DBP_3, bias.variance.synCF.DBP_3, bias.variance.BARTsynth.DBP_3, bias.variance.BARTsynth2.DBP_3))
names <- c("BART", "BART2", "synth", "synCF", "BARTsynth", "BARTsynth2")
rownames(bias.variances.DBP_3) <- names
colnames(bias.variances.DBP_3) <- c("bias2", "variance")
plot(bias.variances.DBP_3, xlab="average squared bias", main="variance vs bias of estimators predicting DBP_3", xlim = c(89, 113), xaxp=c(90, 112, 22))
text(bias.variances.DBP_3, labels = names, pos = c(4,2,2,4,4,3))
bias.variance.APSE.DBP_3 = cbind(bias.variances.DBP_3, APSE=bias.variances.DBP_3[,1]+bias.variances.DBP_3[,2])
```

```{r, echo=FALSE, eval=FALSE}
bias.variances.MAP_3 <- t(cbind(bias.variance.BART.MAP_3, bias.variance.BART2.MAP_3, bias.variance.synth.MAP_3, bias.variance.synCF.MAP_3, bias.variance.BARTsynth.MAP_3, bias.variance.BARTsynth2.MAP_3))
names <- c("BART", "BART2", "synth", "synCF", "BARTsynth", "BARTsynth2")
rownames(bias.variances.MAP_3) <- names
colnames(bias.variances.MAP_3) <- c("bias2", "variance")
plot(bias.variances.MAP_3, xlab="average squared bias", main="variance vs bias of estimators predicting MAP_3", xlim = c(84, 103), xaxp=c(85, 102, 17))
text(bias.variances.MAP_3, labels = names, pos = c(4,2,3,4,4,3))
bias.variance.APSE.MAP_3 = cbind(bias.variances.MAP_3, APSE=bias.variances.MAP_3[,1]+bias.variances.MAP_3[,2])
```

```{r, echo=FALSE, eval=FALSE}
bias.variances.SBP_3 <- t(cbind(bias.variance.BART.SBP_3, bias.variance.BART2.SBP_3, bias.variance.synth.SBP_3, bias.variance.synCF.SBP_3, bias.variance.BARTsynth.SBP_3, bias.variance.BARTsynth2.SBP_3))
names <- c("BART", "BART2", "synth", "synCF", "BARTsynth", "BARTsynth2")
rownames(bias.variances.SBP_3) <- names
colnames(bias.variances.SBP_3) <- c("bias2", "variance")
plot(bias.variances.SBP_3, xlab="average squared bias", main="variance vs bias of estimators predicting SBP_3", xlim = c(145, 190), xaxp=c(150, 190, 4))
text(bias.variances.SBP_3, labels = names, pos = c(2,4,2,2,4,3))
bias.variance.APSE.SBP_3 = cbind(bias.variances.SBP_3, APSE=bias.variances.SBP_3[,1]+bias.variances.SBP_3[,2])
```

```{r, echo=FALSE, eval=FALSE}
bias.variances.RR_1 <- t(cbind(bias.variance.BART.RR_1, bias.variance.BART2.RR_1, bias.variance.synth.RR_1, bias.variance.synCF.RR_1, bias.variance.BARTsynth.RR_1, bias.variance.BARTsynth2.RR_1))
names <- c("BART", "BART2", "synth", "synCF", "BARTsynth", "BARTsynth2")
rownames(bias.variances.RR_1) <- names
colnames(bias.variances.RR_1) <- c("bias2", "variance")
plot(bias.variances.RR_1, xlab="average squared bias", main="variance vs bias of estimators predicting RR_1", xlim = c(26, 33), xaxp=c(27, 32, 5))
text(bias.variances.RR_1, labels = names, pos = c(2,2,2,2,4,3))
bias.variance.APSE.RR_1 = cbind(bias.variances.RR_1, APSE=bias.variances.RR_1[,1]+bias.variances.RR_1[,2])
```

```{r, echo=FALSE, eval=FALSE}
bias.variances.TV_1 <- t(cbind(bias.variance.BART.TV_1, bias.variance.BART2.TV_1, bias.variance.synth.TV_1, bias.variance.synCF.TV_1, bias.variance.BARTsynth.TV_1, bias.variance.BARTsynth2.TV_1))
names <- c("BART", "BART2", "synth", "synCF", "BARTsynth", "BARTsynth2")
rownames(bias.variances.TV_1) <- names
colnames(bias.variances.TV_1) <- c("bias2", "variance")
plot(bias.variances.TV_1, xlab="average squared bias", main="variance vs bias of estimators predicting TV_1", xlim = c(124100, 132900), xaxp=c(124200, 132800, 5))
text(bias.variances.TV_1, labels = names, pos = c(4,2,2,2,2,3))
bias.variance.APSE.TV_1 = cbind(bias.variances.TV_1, APSE=bias.variances.TV_1[,1]+bias.variances.TV_1[,2])
```


```{r, echo=FALSE, eval=FALSE}
bias.variances.TV_1.5 <- t(cbind(bias.variance.BART.TV_1.5, bias.variance.BART2.TV_1.5, bias.variance.synth.TV_1.5, bias.variance.synCF.TV_1.5, bias.variance.BARTsynth.TV_1.5, bias.variance.BARTsynth2.TV_1.5))
names <- c("BART", "BART2", "synth", "synCF", "BARTsynth", "BARTsynth2")
rownames(bias.variances.TV_1.5) <- names
colnames(bias.variances.TV_1.5) <- c("bias2", "variance")
plot(bias.variances.TV_1.5, xlab="average squared bias", main="variance vs bias of estimators predicting TV_1^.5", xlim = c(27, 30), xaxp=c(27, 30, 6))
text(bias.variances.TV_1.5, labels = names, pos = c(2,4,2,4,4,3))
bias.variance.APSE.TV_1.5 = cbind(bias.variances.TV_1.5, APSE=bias.variances.TV_1.5[,1]+bias.variances.TV_1.5[,2])
```


# References